ğŸš€ LangGraph RAG Engine

A fast, modular Retrieval-Augmented Generation (RAG) system using LangChain, LangGraph, FAISS, and OpenAI/Ollama/OpenRouter-compatible LLMs.

This project builds a production-style RAG pipeline with:

PDF, TXT, and URL ingestion

Chunking and embeddings

FAISS vector search

A LangGraph-powered RAG workflow

Optional Agentic RAG (REACT + Tools)

Streamlit UI for interaction

âœ¨ Features
ğŸ“„ Document Ingestion

Supports:

PDF files

Plain text files

URL lists inside .txt files

Entire directory ingestion (data/)

ğŸ§© Smart Text Chunking

Uses RecursiveCharacterTextSplitter for clean document splitting with overlap.

ğŸ” Embedding + FAISS Vector Store

Fast semantic search

Local CPU-friendly

Vector retriever auto-integrated with LangGraph nodes

ğŸ”— LangGraph Workflow

Custom RAG pipeline:

[USER QUESTION]
       â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Retriever  â”‚ â†’ selects top-k document chunks
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ LLM Answer â”‚ â†’ generates context-grounded answer
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   Final Output

ğŸ¤– Agentic RAG (optional)

REACT agent with tools:

Vector store retriever

Wikipedia search

Enhanced answer reasoning

ğŸ–¥ï¸ Streamlit UI

Simple and clean:

Input box

Result display

Source document preview

History of queries

ğŸ“ Project Structure
Project1/
â”‚
â”œâ”€â”€ streamlit.py                 # Main UI application
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                         # API keys (ignored in git)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.py            # LLM model + settings
â”‚   â”‚
â”‚   â”œâ”€â”€ document_ingestion/
â”‚   â”‚   â””â”€â”€ document_processor.py # Load PDF/TXT/URLs + chunking
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â””â”€â”€ vectorstore.py       # Embeddings + FAISS retriever
â”‚   â”‚
â”‚   â”œâ”€â”€ graphbuilder/
â”‚   â”‚   â””â”€â”€ graph_builder.py     # LangGraph RAG pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ nodes.py             # Basic RAG nodes
â”‚   â”‚   â””â”€â”€ reactnode.py         # Agentic RAG node (React + tools)
â”‚   â”‚
â”‚   â””â”€â”€ states/
â”‚       â””â”€â”€ rag_state.py         # Shared state object
â”‚
â””â”€â”€ data/                        # Place PDFs, Text files, URL lists
    â”œâ”€â”€ myfile.pdf
    â”œâ”€â”€ urls.txt
    â””â”€â”€ notes.txt

âš™ï¸ Installation
1ï¸âƒ£ Clone repo
git clone git@github.com:dinesh43431/langgraph-rag-engine.git
cd langgraph-rag-engine

2ï¸âƒ£ Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ”‘ Environment Variables

Create a .env file in the project root:

OPENROUTER_API_KEY=your_key_here


To use Ollama locally:

LLM_MODEL=ollama:llama3

â–¶ï¸ Run the Streamlit App
streamlit run streamlit.py

ğŸ§ª Test RAG from CLI
python test.py

ğŸ§  How RAG Works in This Project

Documents are loaded from the data/ folder

Text is split into overlapping chunks

Embeddings are computed for each chunk

FAISS indexes these vectors

Query â†’ converted to embedding

FAISS retrieves top-k matching chunks

LLM generates answer using ONLY retrieved context

Streamlit displays answer + supporting documents

ğŸ’¡ Example Query
Who is Indiaâ€™s T20I vice-captain?


RAG will:

Find matching chunk in vector DB

Extract only contextually correct info

Prevent hallucinations

ğŸš€ Future Enhancements

Add reranking (BGE, Cohere)

Tool-using agent for multi-step reasoning

Chat memory integration

Upload files directly from UI

Multi-model switching interface
